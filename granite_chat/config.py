import os
from typing import Annotated, Literal

from pydantic import AfterValidator, Field, TypeAdapter, model_validator
from pydantic.networks import HttpUrl
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    port: int = Field(default=8000, description="HTTP Port the agent will listen on")

    host: str = Field(default="127.0.0.1", description="Network address the agent will bind to")
    ACCESS_LOG: bool = Field(default=False, description="Whether the agent logs HTTP access requests")

    STREAMING: bool = Field(default=True, description="Stream user facing content")

    LLM_PROVIDER: Literal["openai", "watsonx"] = "openai"
    LLM_MODEL: str | None = Field(description="The model ID of the LLM")
    LLM_API_BASE: Annotated[
        HttpUrl | None, Field(description="The OpenAI base URL for chat completions"), AfterValidator(str)
    ] = None
    LLM_API_KEY: str | None = Field(
        description="The authorization key used to access LLM_MODEL via LLM_API_BASE", default=None
    )
    LLM_API_HEADERS: str | None = Field(description="Additional headers to provide to LLM_API_BASE", default=None)

    RETRIEVER: Literal["google", "tavily"] = Field(default="google", description="The search engine to use")
    GOOGLE_API_KEY: str | None = Field(description="The API key for Google Search")
    GOOGLE_CX_KEY: str | None = Field(description="The CX key for Google Search")
    TAVILY_API_KEY: str | None = Field(default=None, description="The API key for Tavily")
    SAFE_SEARCH: bool = Field(default=True, description="Turn on safe search if available for search engine.")

    OLLAMA_BASE_URL: Annotated[
        HttpUrl,
        Field(
            default_factory=lambda: TypeAdapter(HttpUrl).validate_python("http://localhost:11434"),
            description="The OpenAI base URL for chat completions",
        ),
        AfterValidator(str),
    ]

    EMBEDDINGS_PROVIDER: Literal["watsonx", "ollama", "openai"] = Field(
        default="watsonx", description="Which provider to use for calculating embeddings"
    )

    EMBEDDINGS_MODEL: str = Field(
        default="ibm/slate-125m-english-rtrvr-v2", description="The model ID of the embedding model"
    )
    EMBEDDINGS_HF_TOKENIZER: str = Field(default="FacebookAI/roberta-base", description="The model ID of the tokenizer")
    CHUNK_SIZE: int = Field(
        default=500,
        description="The maximum number of characters search result chunks will be broken into",
        ge=10,
        le=8192,
    )

    CHUNK_OVERLAP: int = Field(default=20, description="The number of characters search result chunks will overlap")
    MAX_EMBEDDINGS: int = Field(default=100, description="The max number of embeddings in a single request")

    # Populate these vars to enable lora citations via granite-io
    # Otherwise agent will fall back on default implementation
    GRANITE_IO_OPENAI_API_BASE: Annotated[
        HttpUrl | None, Field(default=None, description="The OpenAI base URL for chat completions"), AfterValidator(str)
    ]
    GRANITE_IO_CITATIONS_MODEL_ID: str | None = Field(default=None, description="The model ID for citations")
    GRANITE_IO_OPENAI_API_HEADERS: str | None = Field(
        default=None, description="Additional headers to provide to GRANITE_IO_OPENAI_API_BASE"
    )

    # WATSONX EMBEDDINGS
    # Setting WATSONX_EMBEDDING_MODEL will override default embedding settings
    WATSONX_API_BASE: Annotated[
        HttpUrl | None, Field(default=None, description="The OpenAI base URL for chat completions"), AfterValidator(str)
    ]
    WATSONX_PROJECT_ID: str | None = Field(default=None, description="The project ID of your Watsonx deployment")
    WATSONX_REGION: str | None = Field(default=None, description="The region of your Watsonx deployment")
    WATSONX_API_KEY: str | None = Field(default=None, description="The Cloud API Key to reach your Watsonx deployment")

    # openai embeddings
    EMBEDDINGS_OPENAI_API_BASE: Annotated[
        HttpUrl | None, Field(default=None, description="The OpenAI base URL for chat completions"), AfterValidator(str)
    ]
    EMBEDDINGS_OPENAI_API_KEY: str | None = Field(default=None, description="The API key to reach your OpenAI endpoint")
    EMBEDDINGS_OPENAI_API_HEADERS: str | None = Field(
        default=None, description="Additional headers to provide to EMBEDDINGS_OPENAI_API_BASE"
    )

    MAX_TOKENS: int = Field(
        default=4096, description="The maximum number of tokens the LLM will generate", ge=10, le=128_000
    )
    TEMPERATURE: float = Field(
        default=0.1,
        description="How predictable (low value) or creative (high value) the LLM responses are",
        ge=0.0,
        le=2.0,
    )

    CHAT_TOKEN_LIMIT: int = Field(
        default=5_000,
        description="The number of tokens that are generated by the LLM before the agent sends a message",
    )

    log_level: Literal["FATAL", "ERROR", "WARNING", "INFO", "DEBUG", "TRACE"] = Field(
        default="INFO", description="Set the log level for the agent"
    )

    # Research configuration
    RESEARCH_PLAN_BREADTH: int = Field(default=5, description="Controls how many search queries are executed", ge=1)
    RESEARCH_MAX_SEARCH_RESULTS_PER_STEP: int = Field(
        default=10, description="Controls how man search results are considered for each search query", ge=1
    )
    RESEARCH_MAX_DOCS_PER_STEP: int = Field(
        default=15, description="The number of documents to return from the vector store"
    )

    # Inference throttle
    MAX_CONCURRENT_INFERENCE_TASKS: int = Field(
        default=20,
        description="The max. number of inference operations that can run simultaneously, includes chat and embeddings",
    )
    RATE_LIMIT_INFERENCE_TASKS: int = Field(
        default=8, description="Rate limit for inference tasks in specified rate period"
    )
    RATE_PERIOD_INFERENCE_TASKS: int = Field(
        default=2, description="Rate period in seconds, use with rate limit to implement throttle"
    )

    # General task throttle
    MAX_CONCURRENT_TASKS: int = Field(
        default=20,
        description="The max. number of tasks that can run simultaneously, excludes inference tasks",
    )
    RATE_LIMIT_TASKS: int = Field(
        default=20,
        description="Rate limit for tasks in specified rate period",
    )
    RATE_PERIOD_TASKS: int = Field(
        default=2, description="Rate period in seconds, use with rate limit to implement throttle"
    )

    # Resource store
    RESOURCE_STORE_PROVIDER: Literal["S3"] | None = None
    S3_BUCKET: str | None = Field(default=None, description="S3 bucket name")
    S3_ENDPOINT: str | None = Field(default=None, description="S3 resource store endpoint")
    S3_ACCESS_KEY_ID: str | None = Field(default=None, description="S3 access key id")
    S3_SECRET_ACCESS_KEY: str | None = Field(default=None, description="S3 secret access ket")

    # Key Store
    KEY_STORE_PROVIDER: Literal["redis"] | None = None
    REDIS_CLIENT_URL: str | None = Field(default=None, description="Redis client object configured from the given URL")

    # MMR
    MMR_LAMBDA_MULT: float = Field(
        default=0.4,
        description="Controls the weighting between relevance and diversity in MMR",
    )

    @model_validator(mode="after")
    def set_secondary_env(self) -> "Settings":
        # We need OLLAMA_BASE_URL to be set in the event that ollama embeddings are used
        if "OLLAMA_BASE_URL" not in os.environ and self.EMBEDDINGS_PROVIDER == "ollama":
            os.environ["OLLAMA_BASE_URL"] = str(self.OLLAMA_BASE_URL)

        # We need RETRIEVER to be set
        if "RETRIEVER" not in os.environ:
            os.environ["RETRIEVER"] = self.RETRIEVER

        if self.RETRIEVER == "google" and (self.GOOGLE_API_KEY is None or self.GOOGLE_CX_KEY is None):
            raise ValueError("Google retriever requires GOOGLE_API_KEY and GOOGLE_CX_KEY")
        elif self.RETRIEVER == "tavily" and self.TAVILY_API_KEY is None:
            raise ValueError("Tavily retriever requires TAVILY_API_KEY")

        # Allows headers to be picked up by framework
        if self.LLM_API_HEADERS:
            os.environ["OPENAI_API_HEADERS"] = self.LLM_API_HEADERS

        return self

    class Config:
        env_file = ".env"


settings = Settings()  # type: ignore[call-arg]
